{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fjmscojFn6"
      },
      "source": [
        "# 0.初めに\n",
        "このノートブックでは学習済みモデルの精度を実際に確認できるようにしてあります。\n",
        "もし学習済みモデルをダウンロードしていない場合はhttps://drive.google.com/file/d/1-4dLB4T9h0XP-rpvYnbXvxBBnXW1vegi/view?usp=sharing\n",
        "からダウンロードして、google driveのマイドライブ直下に保存してください。\n",
        "\n",
        "このノートブックは1.モデルを試す　からが本質になっているため、そこまではとりあえず実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcUeSyodUGZi"
      },
      "source": [
        "#このセルはとりあえず実行してください\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "!git checkout efdb46b6e2a53f9126d447260f916cac33de58c3\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir apex/\n",
        "!pip install mecab-python3==0.996.6rc2\n",
        "!pip install transformers==2.8.0\n",
        "!git clone https://github.com/kuma807/bert_qa.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKqrLp9Bkj-a",
        "outputId": "4c3dbc87-b98e-4875-f0cb-024a25a2a9bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbdQ84UYohCf"
      },
      "source": [
        "#もしgoogle driveのマイドライブ直下以外に保存した場合model_dirを変更してください\n",
        "model_dir = \"gdrive/My Drive/model/pytorch_model.bin\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmGHlsxeV7K"
      },
      "source": [
        "#モデルと予想用関数の設定\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "config = AutoConfig.from_pretrained(\"bert_qa/config.json\")\n",
        "tokenizer_config = AutoConfig.from_pretrained(\"bert_qa/tokenizer_config.json\")\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bandainamco-mirai/distilbert-base-japanese\", config=config)\n",
        "model.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", config=tokenizer_config)\n",
        "def predict(quesion, text):\n",
        "  input_ids = tokenizer.encode(quesion, text)\n",
        "  token_type_ids = [0 if i <= input_ids.index(3) else 1 for i in range(len(input_ids))]\n",
        "  start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
        "  score = torch.max(start_scores).item() + torch.max(end_scores).item()\n",
        "  all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  prediction = ''.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
        "  prediction = prediction.replace(\"##\", \"\")\n",
        "  return prediction, score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBR2PVy4nbaX"
      },
      "source": [
        "# 1.モデルを試す\n",
        "textにモデルに与える問題文、questionに質問を入れることでモデルの予想を見ることができます。\n",
        "下に2つほど問題文と質問の例を示していますが、基本的にどんな質問でも行けると思うので自分で質問を作ったりして試してみてください。\n",
        "モデルの予想に[CLS]と出た場合それはモデルが問題文に答えがないと判断していると思ってください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QopNmMdqYYbd",
        "outputId": "1aedfcd2-17d1-4e52-beb5-08d824d49949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#例1\n",
        "text = \"\"\"\n",
        "水物である、水が1Lと、牛乳が0.5Lと、オレンジジュースが0.2L、キャベツが0.7個あります。\n",
        "\"\"\"\n",
        "quesion = \"\"\n",
        "\n",
        "prediction, score = predict(quesion, text)\n",
        "print(\"モデルの予想結果:\", prediction)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "モデルの予想結果: [CLS]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKS2nJNMhExp",
        "outputId": "47884bde-7356-4121-fc7d-43c25f429799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#例2\n",
        "text = \"\"\"\n",
        "紀元前10世紀ごろに成立したと見られるヤハウェ信仰者による出典によると、ヤハウェは地が乾きなにも生えていないころに最初にアダムを創造したとされる。\n",
        "ヤハウェは地面の土（アダマ）を使ってアダムの形を作り、鼻の穴からルーアハを吹き込んだ。\n",
        "\"\"\"\n",
        "\n",
        "quesion = \"りんごは何色ですか？\"\n",
        "prediction, score = predict(quesion, text)\n",
        "print(\"モデルの予想結果:\", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "モデルの予想結果: [CLS]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXfidaT7qlcb"
      },
      "source": [
        "例2では問題文中に答えがなくモデルが[CLS]を出力する"
      ]
    }
  ]
}